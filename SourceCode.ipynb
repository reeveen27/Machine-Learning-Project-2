{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5621660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('C:/Users/rjegathe/OneDrive - Intel Corporation/Documents/USM/CDS503/Assignment/Tweets.csv/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fd7279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b733c826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence   airline  \\\n",
       "14635                     NaN                     0.0000  American   \n",
       "14636  Customer Service Issue                     1.0000  American   \n",
       "14637                     NaN                        NaN  American   \n",
       "14638  Customer Service Issue                     0.6659  American   \n",
       "14639                     NaN                     0.0000  American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775e15dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
      "       'negativereason', 'negativereason_confidence', 'airline',\n",
      "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
      "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
      "       'tweet_location', 'user_timezone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e85f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def remove_urls(text):\n",
    "    # Define the pattern for matching URLs\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    # Substitute URLs found in the text with an empty string\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88d550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  @VirginAmerica What @dhepburn said.\n",
      "1     @VirginAmerica plus you've added commercials ...\n",
      "2     @VirginAmerica I didn't today... Must mean I ...\n",
      "3    Bad Flight @VirginAmerica it's really aggressi...\n",
      "4    Can't Tell @VirginAmerica and it's a really bi...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'text' and 'negativereason' are the columns of interest\n",
    "df['combined_text'] = df['negativereason'].fillna('') + ' ' + df['text']\n",
    "\n",
    "# Example of further cleaning steps (e.g., remove URLs)\n",
    "df['cleaned_text'] = df['combined_text'].apply(lambda x: remove_urls(x))\n",
    "\n",
    "# Print out the cleaned data\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ab8efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n",
      "\n",
      "Bad Flight @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "\n",
      "Can't Tell @VirginAmerica and it's a really big bad thing about it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "df['transactions'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Convert the DataFrame series to a list of transactions\n",
    "transactions = df['transactions'].tolist()\n",
    "\n",
    "# Print the first 5 transactions in a neater way\n",
    "for transaction in transactions[:5]:\n",
    "    print(' '.join(transaction))\n",
    "    print()  # Add an empty line between transactions for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1109a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_usernames(text):\n",
    "    # Removes usernames (e.g., @VirginAmerica)\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_html_entities(text):\n",
    "    # Removes HTML entities (e.g., &amp;)\n",
    "    return re.sub(r'&\\w+;', '', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    # Removes non-alphabetic characters\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff0ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cleaning functions to each transaction\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_usernames)\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_html_entities)\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_non_alpha)\n",
    "#df['cleaned_text'] = df['cleaned_text'].apply(remove_stop_words)\n",
    "\n",
    "# Re-tokenize the cleaned text\n",
    "df['transactions'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Convert the DataFrame series to a list of transactions again\n",
    "transactions = df['transactions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e618d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What said\n",
      "\n",
      "plus youve added commercials to the experience tacky\n",
      "\n",
      "I didnt today Must mean I need to take another trip\n",
      "\n",
      "Bad Flight its really aggressive to blast obnoxious entertainment in your guests faces they have little recourse\n",
      "\n",
      "Cant Tell and its a really big bad thing about it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 cleaned transactions in a neater way\n",
    "for transaction in transactions[:5]:\n",
    "    print(' '.join(transaction))\n",
    "    print()  # Add an empty line between transactions for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557b2983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text:\n",
      " 0                  @VirginAmerica What @dhepburn said.\n",
      "1     @VirginAmerica plus you've added commercials ...\n",
      "2     @VirginAmerica I didn't today... Must mean I ...\n",
      "3    Bad Flight @VirginAmerica it's really aggressi...\n",
      "4    Can't Tell @VirginAmerica and it's a really bi...\n",
      "Name: combined_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['combined_text'] = df['negativereason'].fillna('') + ' ' + df['text']\n",
    "print(\"Combined Text:\\n\", df['combined_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f59c95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rjegathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rjegathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to run this cell first to import nltk and download the necessary resources\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Downloading the 'punkt' tokenizer model and 'stopwords' dataset from nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e6857a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (2.10.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a996f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab675b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rjegathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rjegathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after removing usernames, URLs, and stop words:\n",
      " 0                                               said .\n",
      "1    plus 've added commercials experience ... tacky .\n",
      "2     n't today ... Must mean need take another trip !\n",
      "3    Bad Flight 's really aggressive blast obnoxiou...\n",
      "4                  Ca n't Tell 's really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_usernames(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return ' '.join([word for word in word_tokens if word.lower() not in stop_words])\n",
    "\n",
    "# Apply the functions\n",
    "df['cleaned_text'] = df['combined_text'].apply(remove_usernames)\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_urls)\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_stop_words)\n",
    "\n",
    "print(\"Text after removing usernames, URLs, and stop words:\\n\", df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff7f04c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                               said .\n",
      "1    plus 've added commercials experience ... tacky .\n",
      "2     n't today ... Must mean need take another trip !\n",
      "3    Bad Flight 's really aggressive blast obnoxiou...\n",
      "4                  Ca n't Tell 's really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern for all emoji characters\n",
    "emoji_pattern = re.compile(\n",
    "    '['\n",
    "    u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "    u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "    u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "    u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "    u'\\U00002702-\\U000027B0'\n",
    "    u'\\U000024C2-\\U0001F251'\n",
    "    ']+',\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to remove emojis\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_emojis)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578b526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after expanding contractions:\n",
      " 0                                               said .\n",
      "1    plus 've added commercials experience ... tacky .\n",
      "2     n't today ... Must mean need take another trip !\n",
      "3    Bad Flight 's really aggressive blast obnoxiou...\n",
      "4                  Ca n't Tell 's really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "contraction_mapping = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "    # Add more contractions as needed\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    for contraction, expanded in contraction_mapping.items():\n",
    "        text = re.sub(contraction, expanded, text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: expand_contractions(x, contraction_mapping))\n",
    "\n",
    "print(\"Text after expanding contractions:\\n\", df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb9857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after separating alphanumeric words:\n",
      " 0                                               said .\n",
      "1    plus 've added commercials experience ... tacky .\n",
      "2     n't today ... Must mean need take another trip !\n",
      "3    Bad Flight 's really aggressive blast obnoxiou...\n",
      "4                  Ca n't Tell 's really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def separate_alphanumeric(text):\n",
    "    return re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', text))\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(separate_alphanumeric)\n",
    "\n",
    "print(\"Text after separating alphanumeric words:\\n\", df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3f8fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after handling repeated characters:\n",
      " 0                                               said .\n",
      "1      plus 've added commercials experience . tacky .\n",
      "2       n't today . Must mean need take another trip !\n",
      "3    Bad Flight 's really aggressive blast obnoxiou...\n",
      "4                  Ca n't Tell 's really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def replace_repeated_characters(text):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(replace_repeated_characters)\n",
    "\n",
    "print(\"Text after handling repeated characters:\\n\", df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "686e648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after filtering out non-alphabetic characters:\n",
      " 0                                                said \n",
      "1         plus ve added commercials experience  tacky \n",
      "2          nt today  Must mean need take another trip \n",
      "3    Bad Flight s really aggressive blast obnoxious...\n",
      "4                    Ca nt Tell s really big bad thing\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_non_alpha(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(remove_non_alpha)\n",
    "\n",
    "print(\"Text after filtering out non-alphabetic characters:\\n\", df['cleaned_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "590eba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (3.7.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rjegathe\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f09c52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support                        itemsets\n",
      "0   0.083402                     (Cancelled)\n",
      "1   0.086885                          (Cant)\n",
      "2   0.200137                      (Customer)\n",
      "3   0.300956                        (Flight)\n",
      "4   0.270902                             (I)\n",
      "..       ...                             ...\n",
      "72  0.198770      (Issue, Customer, Service)\n",
      "73  0.096585           (Issue, Customer, to)\n",
      "74  0.096858         (Customer, Service, to)\n",
      "75  0.096585            (Issue, Service, to)\n",
      "76  0.096585  (Issue, Customer, Service, to)\n",
      "\n",
      "[77 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Instantiate the encoder and encode the transactions\n",
    "encoder = TransactionEncoder()\n",
    "encoded_array = encoder.fit(transactions).transform(transactions)\n",
    "\n",
    "# Convert the encoded array to a DataFrame\n",
    "transaction_df = pd.DataFrame(encoded_array, columns=encoder.columns_)\n",
    "\n",
    "# Use the apriori function from mlxtend to find frequent itemsets\n",
    "frequent_itemsets = apriori(transaction_df, min_support=0.07, use_colnames=True)\n",
    "\n",
    "# Print the frequent itemsets\n",
    "print(frequent_itemsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4892563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    antecedents   consequents  antecedent support  consequent support  \\\n",
      "0   (Attendant)  (Complaints)            0.032923            0.032855   \n",
      "1  (Complaints)   (Attendant)            0.032855            0.032923   \n",
      "2   (Attendant)      (Flight)            0.032923            0.300956   \n",
      "3         (Bad)      (Flight)            0.040779            0.300956   \n",
      "4     (Booking)      (Flight)            0.043852            0.300956   \n",
      "\n",
      "    support  confidence       lift  leverage  conviction  zhangs_metric  \n",
      "0  0.032855    0.997925  30.373444  0.031773  466.163798       1.000000  \n",
      "1  0.032855    1.000000  30.373444  0.031773         inf       0.999929  \n",
      "2  0.032855    0.997925   3.315848  0.022947  336.939071       0.722195  \n",
      "3  0.039891    0.978224   3.250387  0.027618   32.102238       0.721777  \n",
      "4  0.042145    0.961059   3.193351  0.028947   17.951443       0.718351  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\n",
    "\n",
    "# Print the rules\n",
    "print(rules.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8b0d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb39a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure 'cleaned_text' is a string, then split it into a list of words (tokens)\n",
    "df['transactions'] = df['cleaned_text'].astype(str).apply(lambda x: x.split())\n",
    "\n",
    "# Now create a list of these transactions\n",
    "transactions = df['transactions'].tolist()\n",
    "\n",
    "# Continue with the TransactionEncoder\n",
    "encoder = TransactionEncoder()\n",
    "onehot_encoded = encoder.fit_transform(transactions)\n",
    "df_onehot = pd.DataFrame(onehot_encoded, columns=encoder.columns_)\n",
    "\n",
    "# Now df_onehot is ready to be used with the fpgrowth function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bcf47ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    support  itemsets\n",
      "0  0.200205      (nt)\n",
      "1  0.301230  (Flight)\n",
      "2  0.095492       (s)\n",
      "3  0.086680      (Ca)\n",
      "4  0.081967    (Tell)\n"
     ]
    }
   ],
   "source": [
    "# Use fpgrowth to find frequent itemsets\n",
    "frequent_itemsets = fpgrowth(df_onehot, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Print the frequent itemsets\n",
    "print(frequent_itemsets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fba55edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  antecedents consequents  antecedent support  consequent support   support  \\\n",
      "0        (Ca)        (nt)            0.086680            0.200205  0.086612   \n",
      "1      (Tell)        (nt)            0.081967            0.200205  0.081421   \n",
      "2      (Tell)        (Ca)            0.081967            0.086680  0.081284   \n",
      "3        (Ca)      (Tell)            0.086680            0.081967  0.081284   \n",
      "4  (Tell, nt)        (Ca)            0.081421            0.086680  0.081284   \n",
      "\n",
      "   confidence       lift  leverage   conviction  zhangs_metric  \n",
      "0    0.999212   4.990946  0.069258  1014.939959       0.875528  \n",
      "1    0.993333   4.961583  0.065011   119.969262       0.869742  \n",
      "2    0.991667  11.440504  0.074179   109.598361       0.994073  \n",
      "3    0.937746  11.440504  0.074179    14.746628       0.999202  \n",
      "4    0.998322  11.517286  0.074227   544.338525       0.994116  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\n",
    "\n",
    "# Print the rules\n",
    "print(rules.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20a4ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            antecedents          consequents  antecedent support  \\\n",
      "0                  (Ca)                 (nt)            0.086680   \n",
      "1                (Tell)                 (nt)            0.081967   \n",
      "2                (Tell)                 (Ca)            0.081967   \n",
      "3                  (Ca)               (Tell)            0.086680   \n",
      "4            (Tell, nt)                 (Ca)            0.081421   \n",
      "5            (Tell, Ca)                 (nt)            0.081284   \n",
      "6              (nt, Ca)               (Tell)            0.086612   \n",
      "7                (Tell)             (nt, Ca)            0.081967   \n",
      "8                  (Ca)           (Tell, nt)            0.086680   \n",
      "9              (flight)             (Flight)            0.196380   \n",
      "10               (Late)             (Flight)            0.129440   \n",
      "11           (Customer)            (Service)            0.200205   \n",
      "12            (Service)           (Customer)            0.200068   \n",
      "13              (Issue)            (Service)            0.199044   \n",
      "14            (Service)              (Issue)            0.200068   \n",
      "15              (Issue)           (Customer)            0.199044   \n",
      "16           (Customer)              (Issue)            0.200205   \n",
      "17    (Issue, Customer)            (Service)            0.198770   \n",
      "18     (Issue, Service)           (Customer)            0.198770   \n",
      "19  (Service, Customer)              (Issue)            0.199317   \n",
      "20              (Issue)  (Service, Customer)            0.199044   \n",
      "21           (Customer)     (Issue, Service)            0.200205   \n",
      "22            (Service)    (Issue, Customer)            0.200068   \n",
      "23          (Cancelled)             (Flight)            0.083402   \n",
      "\n",
      "    consequent support   support  confidence       lift  leverage  \\\n",
      "0             0.200205  0.086612    0.999212   4.990946  0.069258   \n",
      "1             0.200205  0.081421    0.993333   4.961583  0.065011   \n",
      "2             0.086680  0.081284    0.991667  11.440504  0.074179   \n",
      "3             0.081967  0.081284    0.937746  11.440504  0.074179   \n",
      "4             0.086680  0.081284    0.998322  11.517286  0.074227   \n",
      "5             0.200205  0.081284    1.000000   4.994882  0.065011   \n",
      "6             0.081967  0.081284    0.938486  11.449527  0.074185   \n",
      "7             0.086612  0.081284    0.991667  11.449527  0.074185   \n",
      "8             0.081421  0.081284    0.937746  11.517286  0.074227   \n",
      "9             0.301230  0.107036    0.545043   1.809396  0.047880   \n",
      "10            0.301230  0.123156    0.951451   3.158559  0.084165   \n",
      "11            0.200068  0.199317    0.995565   4.976124  0.159262   \n",
      "12            0.200205  0.199317    0.996244   4.976124  0.159262   \n",
      "13            0.200068  0.198770    0.998627   4.991432  0.158948   \n",
      "14            0.199044  0.198770    0.993513   4.991432  0.158948   \n",
      "15            0.200205  0.198770    0.998627   4.988026  0.158921   \n",
      "16            0.199044  0.198770    0.992835   4.988026  0.158921   \n",
      "17            0.200068  0.198770    1.000000   4.998293  0.159003   \n",
      "18            0.200205  0.198770    1.000000   4.994882  0.158976   \n",
      "19            0.199044  0.198770    0.997258   5.010248  0.159098   \n",
      "20            0.199317  0.198770    0.998627   5.010248  0.159098   \n",
      "21            0.198770  0.198770    0.992835   4.994882  0.158976   \n",
      "22            0.198770  0.198770    0.993513   4.998293  0.159003   \n",
      "23            0.301230  0.067281    0.806716   2.678077  0.042158   \n",
      "\n",
      "     conviction  zhangs_metric  \n",
      "0   1014.939959       0.875528  \n",
      "1    119.969262       0.869742  \n",
      "2    109.598361       0.994073  \n",
      "3     14.746628       0.999202  \n",
      "4    544.338525       0.994116  \n",
      "5           inf       0.870558  \n",
      "6     14.923918       0.999203  \n",
      "7    109.606557       0.994148  \n",
      "8     14.755406       0.999840  \n",
      "9      1.535906       0.556643  \n",
      "10    14.393153       0.785012  \n",
      "11   180.353830       0.999056  \n",
      "12   212.963618       0.998886  \n",
      "13   582.750239       0.998377  \n",
      "14   123.473735       0.999656  \n",
      "15   582.650717       0.998207  \n",
      "16   111.790613       0.999656  \n",
      "17          inf       0.998380  \n",
      "18          inf       0.998210  \n",
      "19   292.148805       0.999658  \n",
      "20   583.297609       0.999317  \n",
      "21   111.828747       1.000000  \n",
      "22   123.515854       1.000000  \n",
      "23     3.615249       0.683612  \n"
     ]
    }
   ],
   "source": [
    "# Filter rules based on lift and confidence\n",
    "significant_rules = rules[(rules['lift'] >= 1) & (rules['confidence'] >= 0.5)]\n",
    "\n",
    "# Print the significant rules\n",
    "print(significant_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "314215d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: frozenset({'Ca'}) -> frozenset({'nt'})\n",
      "Support: 0.0866120218579235, Confidence: 0.9992119779353821, Lift: 4.990946215276012\n",
      "\n",
      "Rule: frozenset({'Tell'}) -> frozenset({'nt'})\n",
      "Support: 0.0814207650273224, Confidence: 0.9933333333333334, Lift: 4.96158307744797\n",
      "\n",
      "Rule: frozenset({'Tell'}) -> frozenset({'Ca'})\n",
      "Support: 0.08128415300546447, Confidence: 0.9916666666666667, Lift: 11.440504334121355\n",
      "\n",
      "Rule: frozenset({'Ca'}) -> frozenset({'Tell'})\n",
      "Support: 0.08128415300546447, Confidence: 0.937746256895193, Lift: 11.440504334121355\n",
      "\n",
      "Rule: frozenset({'Tell', 'nt'}) -> frozenset({'Ca'})\n",
      "Support: 0.08128415300546447, Confidence: 0.9983221476510067, Lift: 11.517286242404047\n",
      "\n",
      "Rule: frozenset({'Tell', 'Ca'}) -> frozenset({'nt'})\n",
      "Support: 0.08128415300546447, Confidence: 1.0, Lift: 4.994882292732855\n",
      "\n",
      "Rule: frozenset({'nt', 'Ca'}) -> frozenset({'Tell'})\n",
      "Support: 0.08128415300546447, Confidence: 0.9384858044164037, Lift: 11.449526813880126\n",
      "\n",
      "Rule: frozenset({'Tell'}) -> frozenset({'nt', 'Ca'})\n",
      "Support: 0.08128415300546447, Confidence: 0.9916666666666667, Lift: 11.449526813880126\n",
      "\n",
      "Rule: frozenset({'Ca'}) -> frozenset({'Tell', 'nt'})\n",
      "Support: 0.08128415300546447, Confidence: 0.937746256895193, Lift: 11.51728624240405\n",
      "\n",
      "Rule: frozenset({'flight'}) -> frozenset({'Flight'})\n",
      "Support: 0.10703551912568306, Confidence: 0.5450434782608695, Lift: 1.8093960366755397\n",
      "\n",
      "Rule: frozenset({'Late'}) -> frozenset({'Flight'})\n",
      "Support: 0.12315573770491803, Confidence: 0.9514511873350924, Lift: 3.158559043670239\n",
      "\n",
      "Rule: frozenset({'Customer'}) -> frozenset({'Service'})\n",
      "Support: 0.1993169398907104, Confidence: 0.9955646537018082, Lift: 4.976123772685037\n",
      "\n",
      "Rule: frozenset({'Service'}) -> frozenset({'Customer'})\n",
      "Support: 0.1993169398907104, Confidence: 0.9962444520314101, Lift: 4.976123772685037\n",
      "\n",
      "Rule: frozenset({'Issue'}) -> frozenset({'Service'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9986273164035689, Lift: 4.991431858022619\n",
      "\n",
      "Rule: frozenset({'Service'}) -> frozenset({'Issue'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9935131444178901, Lift: 4.991431858022619\n",
      "\n",
      "Rule: frozenset({'Issue'}) -> frozenset({'Customer'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9986273164035689, Lift: 4.988025899743517\n",
      "\n",
      "Rule: frozenset({'Customer'}) -> frozenset({'Issue'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9928352098259979, Lift: 4.988025899743517\n",
      "\n",
      "Rule: frozenset({'Issue', 'Customer'}) -> frozenset({'Service'})\n",
      "Support: 0.1987704918032787, Confidence: 1.0, Lift: 4.99829293274155\n",
      "\n",
      "Rule: frozenset({'Issue', 'Service'}) -> frozenset({'Customer'})\n",
      "Support: 0.1987704918032787, Confidence: 1.0, Lift: 4.994882292732855\n",
      "\n",
      "Rule: frozenset({'Service', 'Customer'}) -> frozenset({'Issue'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9972583961617546, Lift: 5.010248085040524\n",
      "\n",
      "Rule: frozenset({'Issue'}) -> frozenset({'Service', 'Customer'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9986273164035689, Lift: 5.010248085040524\n",
      "\n",
      "Rule: frozenset({'Customer'}) -> frozenset({'Issue', 'Service'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9928352098259979, Lift: 4.994882292732855\n",
      "\n",
      "Rule: frozenset({'Service'}) -> frozenset({'Issue', 'Customer'})\n",
      "Support: 0.1987704918032787, Confidence: 0.9935131444178901, Lift: 4.99829293274155\n",
      "\n",
      "Rule: frozenset({'Cancelled'}) -> frozenset({'Flight'})\n",
      "Support: 0.06728142076502733, Confidence: 0.8067158067158068, Lift: 2.67807696379125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpret the results (this step is more qualitative and depends on your specific goals)\n",
    "for index, rule in significant_rules.iterrows():\n",
    "    print(f\"Rule: {rule['antecedents']} -> {rule['consequents']}\")\n",
    "    print(f\"Support: {rule['support']}, Confidence: {rule['confidence']}, Lift: {rule['lift']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7d5e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        antecedents         consequents  antecedent support  consequent support  support  confidence      lift\n",
      "               (Ca)                (nt)            0.086680            0.200205 0.086612    0.999212  4.990946\n",
      "             (Tell)                (nt)            0.081967            0.200205 0.081421    0.993333  4.961583\n",
      "             (Tell)                (Ca)            0.081967            0.086680 0.081284    0.991667 11.440504\n",
      "               (Ca)              (Tell)            0.086680            0.081967 0.081284    0.937746 11.440504\n",
      "         (Tell, nt)                (Ca)            0.081421            0.086680 0.081284    0.998322 11.517286\n",
      "         (Tell, Ca)                (nt)            0.081284            0.200205 0.081284    1.000000  4.994882\n",
      "           (nt, Ca)              (Tell)            0.086612            0.081967 0.081284    0.938486 11.449527\n",
      "             (Tell)            (nt, Ca)            0.081967            0.086612 0.081284    0.991667 11.449527\n",
      "               (Ca)          (Tell, nt)            0.086680            0.081421 0.081284    0.937746 11.517286\n",
      "           (flight)            (Flight)            0.196380            0.301230 0.107036    0.545043  1.809396\n",
      "             (Late)            (Flight)            0.129440            0.301230 0.123156    0.951451  3.158559\n",
      "         (Customer)           (Service)            0.200205            0.200068 0.199317    0.995565  4.976124\n",
      "          (Service)          (Customer)            0.200068            0.200205 0.199317    0.996244  4.976124\n",
      "            (Issue)           (Service)            0.199044            0.200068 0.198770    0.998627  4.991432\n",
      "          (Service)             (Issue)            0.200068            0.199044 0.198770    0.993513  4.991432\n",
      "            (Issue)          (Customer)            0.199044            0.200205 0.198770    0.998627  4.988026\n",
      "         (Customer)             (Issue)            0.200205            0.199044 0.198770    0.992835  4.988026\n",
      "  (Issue, Customer)           (Service)            0.198770            0.200068 0.198770    1.000000  4.998293\n",
      "   (Issue, Service)          (Customer)            0.198770            0.200205 0.198770    1.000000  4.994882\n",
      "(Service, Customer)             (Issue)            0.199317            0.199044 0.198770    0.997258  5.010248\n",
      "            (Issue) (Service, Customer)            0.199044            0.199317 0.198770    0.998627  5.010248\n",
      "         (Customer)    (Issue, Service)            0.200205            0.198770 0.198770    0.992835  4.994882\n",
      "          (Service)   (Issue, Customer)            0.200068            0.198770 0.198770    0.993513  4.998293\n",
      "        (Cancelled)            (Flight)            0.083402            0.301230 0.067281    0.806716  2.678077\n"
     ]
    }
   ],
   "source": [
    "# Assume 'significant_rules' is a DataFrame containing the rules with their metrics\n",
    "# We'll select a few relevant columns to display\n",
    "columns_to_display = ['antecedents', 'consequents', 'antecedent support', 'consequent support', 'support', 'confidence', 'lift']\n",
    "\n",
    "# Set the maximum number of rows to display\n",
    "pd.options.display.max_rows = 10  # Change this number based on your preference\n",
    "\n",
    "# Convert the DataFrame to a string and print it\n",
    "print(significant_rules[columns_to_display].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87189313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcea765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
